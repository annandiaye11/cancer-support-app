// Configuration centralis√©e pour l'IA Psychologue
// Modifiez ce fichier pour changer de provider sans toucher au code

export const AI_CONFIG = {
  // Provider actif : 'openai' | 'anthropic' | 'ollama' | 'simulation'
  provider: (process.env.AI_PROVIDER || 'simulation') as 'openai' | 'anthropic' | 'ollama' | 'simulation',
  
  // Mod√®le √† utiliser
  model: process.env.AI_MODEL || 'simulation',
  
  // Param√®tres de g√©n√©ration
  temperature: parseFloat(process.env.AI_TEMPERATURE || '0.7'),
  maxTokens: parseInt(process.env.AI_MAX_TOKENS || '300'),
  
  // Cl√©s API (automatiquement charg√©es depuis .env.local)
  apiKeys: {
    openai: process.env.OPENAI_API_KEY,
    anthropic: process.env.ANTHROPIC_API_KEY,
  },
  
  // Configuration Ollama local
  ollama: {
    baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
  },
}

// V√©rification de la configuration
export function validateAIConfig() {
  const { provider, apiKeys } = AI_CONFIG
  
  if (provider === 'openai' && !apiKeys.openai) {
    console.warn('‚ö†Ô∏è  OPENAI_API_KEY non configur√©e. Mode simulation activ√©.')
    return false
  }
  
  if (provider === 'anthropic' && !apiKeys.anthropic) {
    console.warn('‚ö†Ô∏è  ANTHROPIC_API_KEY non configur√©e. Mode simulation activ√©.')
    return false
  }
  
  if (provider === 'ollama') {
    console.log('ü¶ô Ollama configur√© sur:', AI_CONFIG.ollama.baseUrl)
  }
  
  return true
}

// Log de d√©marrage
if (process.env.NODE_ENV === 'development') {
  console.log('ü§ñ IA Psy Provider:', AI_CONFIG.provider)
  console.log('üéØ Mod√®le:', AI_CONFIG.model)
  validateAIConfig()
}
